{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Survival Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Load Data\n",
    "\n",
    "First, let's import the necessary libraries and load our training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Head:\n",
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "\n",
    "print(\"Training Data Head:\")\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Now, let's explore the data to understand its structure, find patterns, and identify missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initial Data Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n",
      "\n",
      "--- Missing values count ---\n",
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Initial Data Info ---\")\n",
    "train_df.info()\n",
    "\n",
    "print(\"\\n--- Missing values count ---\")\n",
    "print(train_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning & Feature Engineering\n",
    "\n",
    "Based on our EDA, we'll clean the data by handling missing values and create new features to improve our model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature preparation complete.\n",
      "   Pclass  Sex   Age  SibSp  Parch     Fare  Embarked_C  Embarked_Q  \\\n",
      "0       3    0  22.0      1      0   7.2500       False       False   \n",
      "1       1    1  38.0      1      0  71.2833        True       False   \n",
      "2       3    1  26.0      0      0   7.9250       False       False   \n",
      "3       1    1  35.0      1      0  53.1000       False       False   \n",
      "4       3    0  35.0      0      0   8.0500       False       False   \n",
      "\n",
      "   Embarked_S  \n",
      "0        True  \n",
      "1       False  \n",
      "2        True  \n",
      "3        True  \n",
      "4        True  \n"
     ]
    }
   ],
   "source": [
    "train_df['Age'].fillna(train_df.groupby(['Pclass', 'Sex'])['Age'].transform('median'), inplace=True)\n",
    "test_df['Age'].fillna(test_df.groupby(['Pclass', 'Sex'])['Age'].transform('median'), inplace=True)\n",
    "train_df['Embarked'].fillna(train_df['Embarked'].mode()[0], inplace=True)\n",
    "test_df['Fare'].fillna(test_df['Fare'].median(), inplace=True)\n",
    "\n",
    "train_df['Sex'] = train_df['Sex'].map({'male': 0, 'female': 1})\n",
    "test_df['Sex'] = test_df['Sex'].map({'male': 0, 'female': 1})\n",
    "train_df = pd.get_dummies(train_df, columns=['Embarked'])\n",
    "test_df = pd.get_dummies(test_df, columns=['Embarked'])\n",
    "\n",
    "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_C', 'Embarked_Q', 'Embarked_S']\n",
    "X_train = train_df[features]\n",
    "y_train = train_df['Survived']\n",
    "X_test = test_df[features]\n",
    "\n",
    "print(\"Feature preparation complete.\")\n",
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training and Evaluation\n",
    "\n",
    "It's time to choose a model, train it on our processed data, and see how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold CV accuracy: 0.8294\n",
      "Model training and evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=300, max_depth=6, random_state=42)\n",
    "gb = GradientBoostingClassifier(n_estimators=200, learning_rate=0.05, max_depth=3, random_state=42)\n",
    "svc = make_pipeline(StandardScaler(), SVC(probability=True, kernel='rbf', C=1.0))\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "\n",
    "estimators = [\n",
    "    ('rf', rf),\n",
    "    ('gb', gb),\n",
    "    ('svc', svc)\n",
    "]\n",
    "stack = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    final_estimator=lr,\n",
    "    passthrough=False,\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_score = cross_val_score(stack, X_train, y_train, cv=kf, scoring='accuracy').mean()\n",
    "print(f\"5-fold CV accuracy: {cv_score:.4f}\")\n",
    "\n",
    "stack.fit(X_train, y_train)\n",
    "\n",
    "print(\"Model training and evaluation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Submission File\n",
    "\n",
    "Finally, we'll use our trained model to make predictions on the test set and generate the submission file in the required format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " it's finished!\n"
     ]
    }
   ],
   "source": [
    "pred = stack.predict(X_test)\n",
    "pred = pred.astype(int)\n",
    "submission = pd.DataFrame({'PassengerId': test_df['PassengerId'], 'Survived': pred})\n",
    "submission.to_csv('submission4.csv', index=False)\n",
    "print(\" it's finished!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
